{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wordcloud\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews.csv',encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.set_index('review_stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stars = df.set_index('review_stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stars.loc['1.0 out of 5 stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stars.loc['5.0 out of 5 stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_body'][:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['review_body']\n",
    "reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using wordcloud to see a visual representation of text body, used to depict keyword.\n",
    "cloud = wordcloud.WordCloud(background_color='black', max_font_size=45, \n",
    "                                relative_scaling=1).generate(' '.join(df_work.review_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "plt.axis('off')\n",
    "plt.imshow(cloud);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk\n",
    "nltk.download('stopwords')\n",
    "stops = stopwords.words('english')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    no_punc = []\n",
    "    for review in tokenized:\n",
    "        line = \"\".join(char for char in review if char not in string.punctuation)\n",
    "        no_punc.append(line)\n",
    "    tokens = lemmatize(no_punc)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemma = [lmtzr.lemmatize(t) for t in tokens]\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.apply(lambda x: tokenize(x))\n",
    "# Remove punctuations, stopwords, and lemmatize\n",
    "reviews[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_stars'] = df['review_stars'].replace('5.0 out of 5 stars',int(5))\n",
    "df['review_stars'] = df['review_stars'].replace('4.0 out of 5 stars',int(4))\n",
    "df['review_stars'] = df['review_stars'].replace('3.0 out of 5 stars',int(3))\n",
    "df['review_stars'] = df['review_stars'].replace('2.0 out of 5 stars',int(2))\n",
    "df['review_stars'] = df['review_stars'].replace('1.0 out of 5 stars',int(1))\n",
    "\n",
    "df.to_csv(\"reviews_work.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews_work.csv',encoding='latin-1')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df_work = df[df['review_stars'] != 3]\n",
    "# Take 3 stars away because they are neutral and did not provide us any useful information.\n",
    "df_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column positivity to encode 1 if review stars > 3, other wise encode 0.\n",
    "df_work['positivity'] = [1 if x > 3 else 0 for x in df_work.review_stars]\n",
    "df_work.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_body = df_work[\"review_body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test Split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_work.review_body, df_work.positivity, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train shape: {}\".format(x_train.shape), end='\\n')\n",
    "print(\"y_train shape: {}\".format(y_train.shape), end='\\n\\n')\n",
    "print(\"x_test shape: {}\".format(x_test.shape), end='\\n')\n",
    "print(\"y_test shape: {}\".format(y_test.shape), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize X_train\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=5).fit(x_train)\n",
    "X_train = vectorizer.transform(x_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tey to build Logistic Regression Model\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.3f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "\n",
    "X_test = vectorizer.transform(x_test)\n",
    "\n",
    "log_y_pred = logreg.predict(X_test)\n",
    "\n",
    "logreg_score = accuracy_score(y_test, log_y_pred)\n",
    "print(\"Accuracy:   {:.3f}\".format(logreg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cfm = confusion_matrix(y_test, log_y_pred)\n",
    "print(\"Confusion matrix:\")\n",
    "print(log_cfm, end='\\n\\n')\n",
    "print('-'*15)\n",
    "print(np.array([['TN', 'FP'],[ 'FN' , 'TP']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(log_cfm, interpolation='nearest')\n",
    "\n",
    "for i, j in itertools.product(range(log_cfm.shape[0]), range(log_cfm.shape[1])):\n",
    "    plt.text(j, i, log_cfm[i, j],\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"red\")\n",
    "\n",
    "plt.ylabel('True label (Recall)')\n",
    "plt.xlabel('Predicted label (Precision)')\n",
    "plt.title('Logistic Reg | Confusion Matrix')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_f1 = f1_score(y_test, log_y_pred)\n",
    "print(\"Logistic Reg - F1 score: {:.3f}\".format(log_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to build Multinomial Bayes Model and to compare with the above one!\n",
    "mnb = MultinomialNB(alpha=.01)\n",
    "mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_y_pred = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_score = accuracy_score(y_test, mnb_y_pred)\n",
    "print(\"Accuracy:   {:.3f}\".format(mnb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.3f}\".format(mnb.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(mnb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_cfm = confusion_matrix(y_test, mnb_y_pred)\n",
    "print(\"Confusion matrix:\")\n",
    "print(mnb_cfm, end='\\n\\n')\n",
    "print('-'*15)\n",
    "print(np.array([['TN', 'FP'],[ 'FN' , 'TP']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mnb_cfm, interpolation='nearest')\n",
    "\n",
    "for i, j in itertools.product(range(mnb_cfm.shape[0]), range(mnb_cfm.shape[1])):\n",
    "    plt.text(j, i, mnb_cfm[i, j],\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"red\")\n",
    "\n",
    "plt.ylabel('True label (Recall)')\n",
    "plt.xlabel('Predicted label (Precision)')\n",
    "plt.title('Multinomial | Confusion Matrix')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_f1 = f1_score(y_test, mnb_y_pred)\n",
    "print(\"Multinomial NB - F1 score: {:.3f}\".format(mnb_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing the accuracy and scores, looks like Multinomial Bayes is the better model to deal with this work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
